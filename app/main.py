"""
AI POC - ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ê²€ìƒ‰ ì±—ë´‡
Streamlit ë©”ì¸ ì•±
"""

import time
import streamlit as st
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

from app.config import (
    APP_TITLE, APP_ICON,
    DEFAULT_SEARCH_RESULTS, DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE,
    LLM_PRICING, SEARCH_PRICING
)
from app.providers.search import get_search_manager, SearchResult
from app.providers.llm import get_llm_manager, LLMResponse
from app.utils.prompt import (
    get_system_prompt,
    build_user_prompt,
    estimate_tokens
)


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout="wide"
)


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "search_results" not in st.session_state:
        st.session_state.search_results = []
    if "last_metrics" not in st.session_state:
        st.session_state.last_metrics = {}


def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # ê²€ìƒ‰ API ì„ íƒ
        st.subheader("ğŸ” ê²€ìƒ‰ API")
        search_manager = get_search_manager()
        available_search = search_manager.get_available_providers()

        search_options = ["DDGS", "Brave", "Tavily", "Google"]
        search_provider = st.selectbox(
            "ê²€ìƒ‰ Provider",
            options=search_options,
            index=0,
            help="ê²€ìƒ‰ì— ì‚¬ìš©í•  API ì„ íƒ"
        )

        # ê²€ìƒ‰ API ìƒíƒœ í‘œì‹œ
        for provider in search_options:
            status = "âœ…" if provider in available_search else "âŒ (í‚¤ ì—†ìŒ)"
            free_limit = SEARCH_PRICING.get(provider, {}).get("free_limit", "")
            st.caption(f"{provider}: {status} - {free_limit}")

        st.divider()

        # LLM ì„ íƒ
        st.subheader("ğŸ¤– LLM")
        llm_manager = get_llm_manager()
        available_llm = llm_manager.get_available_providers()

        llm_options = ["Gemini 2.0 Flash", "GPT-5-nano", "GPT-5-mini", "GPT-4o-mini"]
        llm_provider = st.selectbox(
            "LLM Provider",
            options=llm_options,
            index=0,
            help="ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  LLM ì„ íƒ"
        )

        # LLM ìƒíƒœ ë° ê°€ê²© í‘œì‹œ
        for provider in llm_options:
            status = "âœ…" if provider in available_llm else "âŒ (í‚¤ ì—†ìŒ)"
            pricing = LLM_PRICING.get(provider, {})
            price_str = f"${pricing.get('input', 0)}/{pricing.get('output', 0)}"
            st.caption(f"{provider}: {status} - {price_str}/1M")

        st.divider()

        # ê³ ê¸‰ ì„¤ì •
        with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
            max_results = st.slider(
                "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜",
                min_value=1,
                max_value=10,
                value=DEFAULT_SEARCH_RESULTS
            )

            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=1.0,
                value=DEFAULT_TEMPERATURE,
                step=0.1
            )

            compact_mode = st.checkbox(
                "í† í° ì ˆì•½ ëª¨ë“œ",
                value=False,
                help="í”„ë¡¬í”„íŠ¸ ì••ì¶•ìœ¼ë¡œ ë¹„ìš© ì ˆê°"
            )

            use_fallback = st.checkbox(
                "Fallback ì‚¬ìš©",
                value=True,
                help="ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ API ì‹œë„"
            )

        st.divider()

        # ë©”íŠ¸ë¦­ í‘œì‹œ
        st.subheader("ğŸ“Š ë§ˆì§€ë§‰ ìš”ì²­ ë©”íŠ¸ë¦­")
        metrics = st.session_state.get("last_metrics", {})
        if metrics:
            st.metric("ì‘ë‹µ ì‹œê°„", f"{metrics.get('response_time', 0):.2f}ì´ˆ")
            st.metric("ì…ë ¥ í† í°", metrics.get('input_tokens', 0))
            st.metric("ì¶œë ¥ í† í°", metrics.get('output_tokens', 0))
            st.metric("ì˜ˆìƒ ë¹„ìš©", f"${metrics.get('cost', 0):.6f}")
            st.caption(f"ê²€ìƒ‰: {metrics.get('search_provider', '-')}")
            st.caption(f"LLM: {metrics.get('llm_provider', '-')}")
        else:
            st.caption("ì•„ì§ ìš”ì²­ ì—†ìŒ")

        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.session_state.search_results = []
            st.session_state.last_metrics = {}
            st.rerun()

        return {
            "search_provider": search_provider,
            "llm_provider": llm_provider,
            "max_results": max_results if 'max_results' in dir() else DEFAULT_SEARCH_RESULTS,
            "temperature": temperature if 'temperature' in dir() else DEFAULT_TEMPERATURE,
            "compact_mode": compact_mode if 'compact_mode' in dir() else False,
            "use_fallback": use_fallback if 'use_fallback' in dir() else True,
        }


def render_chat_messages():
    """ì±„íŒ… ë©”ì‹œì§€ ë Œë”ë§"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


def render_search_results(results: list[SearchResult]):
    """ê²€ìƒ‰ ê²°ê³¼ ë Œë”ë§"""
    if not results:
        return

    with st.expander(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´)", expanded=False):
        for i, r in enumerate(results, 1):
            st.markdown(f"""
**[{i}] [{r.title}]({r.url})**
{r.snippet[:200]}{'...' if len(r.snippet) > 200 else ''}
<small>ì¶œì²˜: {r.source}</small>
""", unsafe_allow_html=True)
            if i < len(results):
                st.divider()


def process_query(query: str, settings: dict) -> tuple[str, list[SearchResult], dict]:
    """
    ì§ˆë¬¸ ì²˜ë¦¬: ê²€ìƒ‰ â†’ LLM ë‹µë³€ ìƒì„±

    Returns:
        (ë‹µë³€, ê²€ìƒ‰ ê²°ê³¼, ë©”íŠ¸ë¦­)
    """
    start_time = time.time()

    # 1. ê²€ìƒ‰ ì‹¤í–‰
    search_manager = get_search_manager()
    search_results, used_search = search_manager.search(
        query=query,
        provider_name=settings["search_provider"],
        max_results=settings["max_results"],
        use_fallback=settings["use_fallback"]
    )

    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = get_system_prompt(compact=settings["compact_mode"])
    user_prompt = build_user_prompt(
        query=query,
        search_results=search_results,
        compact=settings["compact_mode"]
    )

    # 3. LLM ë‹µë³€ ìƒì„±
    llm_manager = get_llm_manager()
    response = llm_manager.generate(
        prompt=user_prompt,
        provider_name=settings["llm_provider"],
        system_prompt=system_prompt,
        max_tokens=DEFAULT_MAX_TOKENS,
        temperature=settings["temperature"]
    )

    # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
    elapsed_time = time.time() - start_time
    metrics = {
        "response_time": elapsed_time,
        "input_tokens": response.input_tokens,
        "output_tokens": response.output_tokens,
        "total_tokens": response.total_tokens,
        "cost": response.estimated_cost,
        "search_provider": used_search or settings["search_provider"],
        "llm_provider": settings["llm_provider"],
    }

    return response.content, search_results, metrics


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    init_session_state()

    # íƒ€ì´í‹€
    st.title(f"{APP_ICON} {APP_TITLE}")
    st.caption("ê²€ìƒ‰ API + LLM ì¡°í•© í…ŒìŠ¤íŠ¸ POC")

    # ì‚¬ì´ë“œë°”
    settings = render_sidebar()

    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([2, 1])

    with col1:
        # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
        render_chat_messages()

        # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
        if st.session_state.search_results:
            render_search_results(st.session_state.search_results)

    with col2:
        # í˜„ì¬ ì„¤ì • ìš”ì•½
        st.info(f"""
**í˜„ì¬ ì„¤ì •**
- ê²€ìƒ‰: {settings['search_provider']}
- LLM: {settings['llm_provider']}
- í† í°ì ˆì•½: {'ON' if settings['compact_mode'] else 'OFF'}
""")

    # ì…ë ¥ì°½
    if prompt := st.chat_input("ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, search_results, metrics = process_query(prompt, settings)

                if answer:
                    st.markdown(answer)
                else:
                    st.error("ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

                # ìƒíƒœ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.session_state.search_results = search_results
                st.session_state.last_metrics = metrics

        st.rerun()


if __name__ == "__main__":
    main()
